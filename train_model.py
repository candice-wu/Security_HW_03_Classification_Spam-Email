import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import joblib
import os
import time
import json

# ========== è¨­å®šæª”æ¡ˆè·¯å¾‘ ==========
DATA_PATH = "sms_spam_no_header.csv"
MODELS_DIR = "models"
VECTORIZER_PATH = "tfidf_vectorizer.pkl"
VOCAB_PATH = "vocab.json"

# ========== 1ï¸âƒ£ è¼‰å…¥è³‡æ–™ ==========
print("ğŸ“‚ æª¢æŸ¥è³‡æ–™é›†...")

if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(
        f"âŒ æ‰¾ä¸åˆ° {DATA_PATH}ï¼Œè«‹ç¢ºèªæª”æ¡ˆå·²æ”¾åœ¨å°ˆæ¡ˆè³‡æ–™å¤¾ä¸­ï¼"
    )

print("âœ… æ‰¾åˆ°è³‡æ–™é›†ï¼Œé–‹å§‹è¼‰å…¥...")
data = pd.read_csv(DATA_PATH, encoding="latin-1")

# å˜—è©¦åµæ¸¬æ¬„ä½çµæ§‹ï¼ˆå› è³‡æ–™é›†å¯èƒ½ç„¡æ¨™é¡Œï¼‰
if len(data.columns) >= 2:
    data.columns = ["label", "message"] + list(data.columns[2:])
else:
    raise ValueError("è³‡æ–™æ ¼å¼éŒ¯èª¤ï¼Œéœ€è‡³å°‘æœ‰å…©å€‹æ¬„ä½ï¼šlabel èˆ‡ message")

# åƒ…ä¿ç•™å¿…è¦æ¬„ä½
data = data[["label", "message"]]
print(f"ğŸ“Š è³‡æ–™ç­†æ•¸ï¼š{len(data)}")

# ========== 2ï¸âƒ£ è³‡æ–™å‰è™•ç† ==========
print("\nğŸ§¹ æ¸…ç†èˆ‡è½‰æ›æ¨™ç±¤...")
data["label"] = data["label"].map({"ham": 0, "spam": 1})

# æª¢æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±å€¼
if data.isnull().sum().any():
    print("âš ï¸ ç™¼ç¾ç¼ºå¤±å€¼ï¼Œå°‡è‡ªå‹•ç§»é™¤")
    data = data.dropna()

# ========== 3ï¸âƒ£ åˆ†å‰²è³‡æ–™é›† ==========
print("\nâœ‚ï¸ åˆ†å‰²è¨“ç·´ / æ¸¬è©¦è³‡æ–™...")
X_train, X_test, y_train, y_test = train_test_split(
    data["message"], data["label"], test_size=0.2, random_state=42
)

# ========== 4ï¸âƒ£ å‘é‡åŒ–æ–‡å­— ==========
print("\nğŸ”¤ æ–‡å­—å‘é‡åŒ–ä¸­...")
vectorizer = TfidfVectorizer(stop_words="english")
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)
print(f"âœ… TF-IDF å‘é‡åŒ–å®Œæˆï¼Œç‰¹å¾µæ•¸é‡ï¼š{X_train_tfidf.shape[1]}")

# ========== 5ï¸âƒ£ å»ºç«‹èˆ‡è¨“ç·´å¤šå€‹æ¨¡å‹ ==========
print("\nğŸ¤– æº–å‚™è¨“ç·´å¤šå€‹æ¨¡å‹...")

# å®šç¾©è¦è¨“ç·´çš„æ¨¡å‹
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Naive Bayes": MultinomialNB(),
    "SVM": SVC(probability=True, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
}

# å»ºç«‹å„²å­˜æ¨¡å‹çš„è³‡æ–™å¤¾
if not os.path.exists(MODELS_DIR):
    os.makedirs(MODELS_DIR)

# è¿´åœˆè¨“ç·´ã€è©•ä¼°ä¸¦å„²å­˜æ¯å€‹æ¨¡å‹
for name, model in models.items():
    print(f"\n{'='*20}")
    print(f"ğŸ‹ï¸â€â™€ï¸ é–‹å§‹è¨“ç·´ï¼š{name}")
    print(f"{'='*20}")
    
    start_time = time.time()
    
    # è¨“ç·´æ¨¡å‹
    model.fit(X_train_tfidf, y_train)
    
    end_time = time.time()
    training_time = end_time - start_time
    
    # è©•ä¼°æ¨¡å‹
    y_pred = model.predict(X_test_tfidf)
    
    print(f"\nğŸ“ˆ æ¨¡å‹è©•ä¼°çµæœ ({name}):")
    print(f"æº–ç¢ºç‡ (Accuracy): {accuracy_score(y_test, y_pred):.4f}")
    print(f"è¨“ç·´è€—æ™‚: {training_time:.2f} ç§’")
    print("æ··æ·†çŸ©é™£ï¼š")
    print(confusion_matrix(y_test, y_pred))
    print("\nåˆ†é¡å ±å‘Šï¼š")
    print(classification_report(y_test, y_pred))
    
    # å„²å­˜æ¨¡å‹
    model_path = os.path.join(MODELS_DIR, f"{name.lower().replace(' ', '_')}_model.pkl")
    joblib.dump(model, model_path)
    print(f"ğŸ’¾ æ¨¡å‹å·²å„²å­˜è‡³: {model_path}")

# ========== 6ï¸âƒ£ å„²å­˜å‘é‡åŒ–å·¥å…·èˆ‡å­—å½™è¡¨ ==========
print(f"\n{'='*20}")
print("ğŸ—‚ï¸ å„²å­˜å…±ç”¨å…ƒä»¶...")
print(f"{'='*20}")

# å„²å­˜å‘é‡åŒ–å·¥å…·
joblib.dump(vectorizer, VECTORIZER_PATH)
print(f"ğŸ“ å·²å„²å­˜å‘é‡å™¨è‡³ï¼š {VECTORIZER_PATH}")

# å„²å­˜å­—å½™è¡¨
vocab = vectorizer.get_feature_names_out()
vocab_dict = {word: idx for idx, word in enumerate(vocab)}
with open(VOCAB_PATH, "w", encoding="utf-8") as f:
    json.dump(vocab_dict, f, ensure_ascii=False, indent=2)
print(f"ğŸ“ å·²å„²å­˜å­—å½™è¡¨è‡³ï¼š {VOCAB_PATH}ï¼ˆå…± {len(vocab_dict)} å€‹è©ï¼‰")


print("\nğŸ‰ æ‰€æœ‰æ¨¡å‹è¨“ç·´å®Œæˆï¼")
print("ğŸ‰ å¯ä»¥åŸ·è¡Œ `streamlit run 5114050013_hw3.py` é–‹å•Ÿæ›´æ–°å¾Œçš„äº’å‹•å¼ç¶²é äº†ï¼")