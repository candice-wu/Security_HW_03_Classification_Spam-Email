import streamlit as st
import pandas as pd
import numpy as np
import joblib
import os
import glob
import time
import random
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, precision_recall_curve
from streamlit_option_menu import option_menu

# ========== åˆå§‹åŒ–è¨­å®š ==========
st.set_page_config(
    page_title="åƒåœ¾éƒµä»¶åˆ†é¡å™¨ Pro",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded",
)

# ========== å…¨åŸŸè·¯å¾‘è¨­å®š ==========
MODELS_DIR = "models"
VECTORIZER_PATH = "tfidf_vectorizer.pkl"
DATA_PATH = "sms_spam_no_header.csv"

# ========== è³‡æºè¼‰å…¥å‡½å¼ (å¿«å–) ==========
@st.cache_data
def load_data():
    """è¼‰å…¥ä¸¦é è™•ç†è³‡æ–™é›†"""
    if not os.path.exists(DATA_PATH):
        st.error(f"âŒ æ‰¾ä¸åˆ°è³‡æ–™é›† {DATA_PATH}ï¼")
        return None
    df = pd.read_csv(DATA_PATH, encoding="latin-1")
    if len(df.columns) >= 2:
        df.columns = ["label", "message"] + list(df.columns[2:])
        df = df[["label", "message"]]
        df["label_num"] = df["label"].map({"ham": 0, "spam": 1})
    else:
        st.error("âš ï¸ CSV çµæ§‹ä¸ç¬¦ï¼Œéœ€åŒ…å« label, message æ¬„ä½ã€‚")
        return None
    return df

@st.cache_resource
def load_vectorizer():
    """è¼‰å…¥ TF-IDF å‘é‡åŒ–å·¥å…·"""
    if not os.path.exists(VECTORIZER_PATH):
        return None
    return joblib.load(VECTORIZER_PATH)

@st.cache_resource
def load_models():
    """å¾ 'models' è³‡æ–™å¤¾è¼‰å…¥æ‰€æœ‰æ¨¡å‹ (Robust version)"""
    models = {}
    model_filenames = {
        "Logistic Regression": "logistic_regression_model.pkl",
        "Naive Bayes": "naive_bayes_model.pkl",
        "SVM": "svm_model.pkl",
        "Random Forest": "random_forest_model.pkl"
    }

    for display_name, filename in model_filenames.items():
        path = os.path.join(MODELS_DIR, filename)
        if os.path.exists(path):
            models[display_name] = joblib.load(path)
        else:
            st.error(f"FATAL: Model file not found at {path}. Deployment is incomplete.")
            return None # Return None to trigger the main error message.
            
    if not models: # If the loop finishes but models dict is empty
        return None

    return models

# ========== å´é‚Šé¸å–® ==========
with st.sidebar:
    st.title("ğŸ¤– åƒåœ¾éƒµä»¶åˆ†é¡å™¨ Pro")
    selected = option_menu(
        menu_title="ä¸»é¸å–®",
        options=["å³æ™‚é æ¸¬", "æ‰¹æ¬¡é æ¸¬", "æ¨¡å‹æ¯”è¼ƒ", "æ¢ç´¢æ€§è³‡æ–™åˆ†æ"],
        icons=["cpu", "upload", "bar-chart-steps", "pie-chart"],
        menu_icon="cast",
        default_index=0,
    )
    
    st.info("""å°ˆæ¡ˆå·²å‡ç´šï¼ç¾åœ¨æ‚¨å¯ä»¥ï¼š
- å³æ™‚æ¸¬è©¦å¤šç¨®æ¨¡å‹
- æ¯”è¼ƒå„æ¨¡å‹æ•ˆèƒ½
- é€²è¡Œæ·±å…¥çš„è³‡æ–™åˆ†æ""")

# ========== è¼‰å…¥æ ¸å¿ƒå…ƒä»¶ ==========
df = load_data()
vectorizer = load_vectorizer()
models = load_models()

if df is None or vectorizer is None or models is None:
    st.error("âŒ æ ¸å¿ƒå…ƒä»¶è¼‰å…¥å¤±æ•—ï¼è«‹å…ˆåŸ·è¡Œ `train_model.py` è¨“ç·´è…³æœ¬ï¼Œä¸¦ç¢ºèªæ¨¡å‹æª”æ¡ˆå·²ä¸Šå‚³è‡³ GitHubã€‚")
    st.stop()

# ========== é é¢è·¯ç”± ==========

# --- 1. å³æ™‚é æ¸¬ (Live Prediction) ---
if selected == "å³æ™‚é æ¸¬":
    st.title("ğŸ’¬ å³æ™‚æ–‡å­—é æ¸¬")

    # è™•ç†å›é¥‹è¨Šæ¯
    if st.session_state.get('feedback_success', False):
        st.balloons()
        st.success("å¤ªæ£’äº†ï¼æ„Ÿè¬æ‚¨çš„æ­£é¢å›é¥‹ã€‚")
        st.session_state.feedback_success = False # é¡¯ç¤ºä¸€æ¬¡å¾Œé‡è¨­

    st.markdown("å¾ä¸‹æ‹‰é¸å–®ä¸­é¸æ“‡ä¸€å€‹æ¨¡å‹ï¼Œç„¶å¾Œè¼¸å…¥æ‚¨æƒ³æ¸¬è©¦çš„è¨Šæ¯ã€‚")

    # åˆå§‹åŒ– session state
    if 'message_input' not in st.session_state:
        st.session_state.message_input = "Congratulations! You've won a $1,000 gift card. Click here to claim."

    def set_example_text(label):
        """Callback function to update the text area's content."""
        sample = df[df['label'] == label].sample(1).iloc[0]
        st.session_state.message_input = sample['message']

    model_name = st.selectbox("**é¸æ“‡æ¨¡å‹**", models.keys())
    
    # å°‡ text_area ç›´æ¥ç¶å®šåˆ° session_state
    st.text_area("**è¼¸å…¥è¦åˆ†é¡çš„è¨Šæ¯ï¼š**", height=150, key="message_input")

    col1, col2 = st.columns(2)
    with col1:
        st.button("ğŸ² è¼‰å…¥æ­£å¸¸éƒµä»¶ç¯„ä¾‹ (Ham)", on_click=set_example_text, args=('ham',), use_container_width=True)
    with col2:
        st.button("ğŸ² è¼‰å…¥åƒåœ¾éƒµä»¶ç¯„ä¾‹ (Spam)", on_click=set_example_text, args=('spam',), use_container_width=True)

    if st.button("ğŸ” åˆ†æè¨Šæ¯", use_container_width=True, type="primary"):
        # å¾ session_state è®€å–è¦åˆ†æçš„è¨Šæ¯
        message_to_analyze = st.session_state.message_input
        if message_to_analyze:
            model = models[model_name]
            message_tfidf = vectorizer.transform([message_to_analyze])
            
            prediction = model.predict(message_tfidf)[0]
            probability = model.predict_proba(message_tfidf)[0]
            
            # å°‡çµæœä¹Ÿå­˜å…¥ session_state ä»¥ä¾¿åœ¨é é¢åˆ·æ–°å¾Œé¡¯ç¤º
            st.session_state.is_spam = (prediction == 1)
            st.session_state.model_name = model_name
            st.session_state.probability = probability[1]
            st.session_state.show_result = True
        else:
            st.warning("è«‹è¼¸å…¥è¨Šæ¯ä»¥é€²è¡Œåˆ†æã€‚")
            st.session_state.show_result = False

    # æ ¹æ“š session_state é¡¯ç¤ºçµæœï¼Œç¢ºä¿é é¢åˆ·æ–°å¾Œçµæœä¾ç„¶å­˜åœ¨
    if st.session_state.get('show_result', False):
        if st.session_state.is_spam:
            st.error(f"**é æ¸¬çµæœï¼šé€™æ˜¯åƒåœ¾è¨Šæ¯ (Spam)**", icon="ğŸš¨")
        else:
            st.success(f"**é æ¸¬çµæœï¼šé€™ä¸æ˜¯åƒåœ¾è¨Šæ¯ (Ham)**", icon="âœ…")
        
        st.metric(label=f"æ¨¡å‹ `{st.session_state.model_name}` é æ¸¬ç‚º Spam çš„æ©Ÿç‡", value=f"{st.session_state.probability:.2%}")

        st.divider()
        st.markdown("#### é€™å€‹é æ¸¬æº–ç¢ºå—ï¼Ÿ")
        feedback_col1, feedback_col2 = st.columns(2)
        with feedback_col1:
            if st.button("ğŸ‘ é æ¸¬æ­£ç¢º", use_container_width=True):
                st.session_state.feedback_success = True
                st.rerun() # ä½¿ç”¨ rerun ç¢ºä¿æ°£çƒç‰¹æ•ˆèƒ½æ­£ç¢ºè§¸ç™¼
        with feedback_col2:
            if st.button("ğŸ‘ é æ¸¬éŒ¯èª¤", use_container_width=True):
                st.toast("å–”ä¸ï¼æ„Ÿè¬æ‚¨çš„å›é¥‹ï¼Œæˆ‘å€‘æœƒåƒè€ƒé€™é …è³‡è¨Šä¾†æ”¹é€²æ¨¡å‹ã€‚")
                st.info("æ‚¨çš„å›é¥‹å°æ–¼æå‡æ¨¡å‹æº–ç¢ºåº¦è‡³é—œé‡è¦ï¼")

    st.divider()
    st.subheader("ğŸ“œ ç¯„ä¾‹è¨Šæ¯")
    filter_option = st.radio("ç¯©é¸è¨Šæ¯é¡åˆ¥", ["All", "Ham", "Spam"])

    if filter_option == "All":
        st.dataframe(df.sample(10))
    else:
        st.dataframe(df[df['label'] == filter_option.lower()].sample(10))

# --- 2. æ‰¹æ¬¡é æ¸¬ (Batch Prediction) ---
elif selected == "æ‰¹æ¬¡é æ¸¬":
    st.title("ğŸ“¤ ä½¿ç”¨ CSV é€²è¡Œæ‰¹æ¬¡é æ¸¬")
    st.markdown("ä¸Šå‚³åŒ…å« `message` æ¬„ä½çš„ CSV æª”æ¡ˆï¼Œé¸æ“‡æ¨¡å‹å¾Œå³å¯é€²è¡Œæ•´æ‰¹é æ¸¬ã€‚")

    model_name = st.selectbox("é¸æ“‡æ¨¡å‹", models.keys())
    
    uploaded_file = st.file_uploader("ä¸Šå‚³æ‚¨çš„ CSV æª”æ¡ˆ", type=["csv"])

    if uploaded_file:
        test_df = pd.read_csv(uploaded_file)
        if "message" in test_df.columns:
            with st.spinner("ğŸ¤– æ­£åœ¨é€²è¡Œæ‰¹æ¬¡é æ¸¬..."):
                model = models[model_name]
                X_test = vectorizer.transform(test_df["message"])
                
                preds = model.predict(X_test)
                probs = model.predict_proba(X_test)[:, 1]
                
                result_df = test_df.copy()
                result_df["prediction"] = ["Spam" if p == 1 else "Ham" for p in preds]
                result_df["spam_probability"] = probs
                
                st.success("âœ… é æ¸¬å®Œæˆï¼")
                st.dataframe(result_df)
                
                @st.cache_data
                def convert_df_to_csv(df_to_convert):
                    return df_to_convert.to_csv(index=False).encode('utf-8')

                csv = convert_df_to_csv(result_df)
                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰é æ¸¬çµæœ",
                    data=csv,
                    file_name=f"predictions_{model_name.replace(' ', '_')}.csv",
                    mime="text/csv",
                )
        else:
            st.error("ä¸Šå‚³çš„ CSV æª”æ¡ˆä¸­å¿…é ˆåŒ…å« `message` æ¬„ä½ã€‚")

# --- 3. æ¨¡å‹æ¯”è¼ƒ (Model Comparison) ---
elif selected == "æ¨¡å‹æ¯”è¼ƒ":
    st.title("ğŸ“Š æ¨¡å‹æ•ˆèƒ½æ¯”è¼ƒ")
    st.markdown("èª¿æ•´ä¸‹æ–¹çš„æ¸¬è©¦é›†æ¯”ä¾‹ï¼Œæ•ˆèƒ½æŒ‡æ¨™å’Œåœ–è¡¨å°‡æœƒè‡ªå‹•æ›´æ–°ã€‚")

    test_size_ratio = st.slider("**å‹•æ…‹èª¿æ•´æ¸¬è©¦é›†æ¯”ä¾‹**", 0.1, 0.5, 0.2, 0.05)

    @st.cache_data
    def get_performance_metrics(test_size):
        X_train, X_test, y_train, y_test = train_test_split(df["message"], df["label_num"], test_size=test_size, random_state=42)
        X_test_tfidf = vectorizer.transform(X_test)
        
        performance = []
        for name, model in models.items():
            start_time = time.time()
            y_pred = model.predict(X_test_tfidf)
            end_time = time.time()
            
            performance.append({
                "Model": name,
                "Accuracy": accuracy_score(y_test, y_pred),
                "Precision": precision_score(y_test, y_pred),
                "Recall": recall_score(y_test, y_pred),
                "F1-Score": f1_score(y_test, y_pred),
                "Prediction Time (s)": end_time - start_time,
            })
        return pd.DataFrame(performance), y_test, X_test_tfidf

    with st.spinner("å‹•æ…‹è©•ä¼°ä¸­ï¼Œè«‹ç¨å€™..."):
        perf_df, y_test_split, X_test_tfidf_split = get_performance_metrics(test_size_ratio)

    st.subheader(f"ğŸ“Š åŸºæ–¼ {test_size_ratio:.0%} æ¸¬è©¦è³‡æ–™çš„æ•ˆèƒ½æŒ‡æ¨™")
    st.dataframe(perf_df.set_index("Model"))

    st.subheader("ğŸ† F1-Score æ¯”è¼ƒåœ–")
    fig, ax = plt.subplots(figsize=(6, 3))
    sns.barplot(data=perf_df, x="F1-Score", y="Model", ax=ax, orient='h', palette="viridis")
    ax.set_xlabel("F1-Score")
    ax.set_ylabel("Model")
    ax.set_title("F1-Score by Model")
    st.pyplot(fig)
    
    st.divider()
    st.subheader("ğŸ¤” æ··æ·†çŸ©é™£ (Confusion Matrix) è©³ç´°åˆ†æ")
    cm_model_name = st.selectbox("**é¸æ“‡ä¸€å€‹æ¨¡å‹ä»¥æŸ¥çœ‹å…¶æ··æ·†çŸ©é™£**", models.keys(), key="cm_select")
    
    if cm_model_name:
        model = models[cm_model_name]
        y_pred = model.predict(X_test_tfidf_split)
        cm = confusion_matrix(y_test_split, y_pred)
        
        fig, ax = plt.subplots(figsize=(6, 4))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax, xticklabels=["Ham", "Spam"], yticklabels=["Ham", "Spam"])
        ax.set_xlabel("Predicted Label")
        ax.set_ylabel("True Label")
        ax.set_title(f"Confusion Matrix for {cm_model_name}")
        st.pyplot(fig)

        # ROC & Precision-Recall Curves
        y_probs = model.predict_proba(X_test_tfidf_split)[:, 1]
        
        col1, col2 = st.columns(2)
        with col1:
            fpr, tpr, _ = roc_curve(y_test_split, y_probs)
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve')
            ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
            ax.set_xlabel('False Positive Rate')
            ax.set_ylabel('True Positive Rate')
            ax.set_title('ROC Curve')
            ax.legend(loc="lower right")
            st.pyplot(fig)

        with col2:
            precision, recall, _ = precision_recall_curve(y_test_split, y_probs)
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')
            ax.set_xlabel('Recall')
            ax.set_ylabel('Precision')
            ax.set_title('Precision-Recall Curve')
            ax.legend(loc="lower left")
            st.pyplot(fig)

        # Threshold Sweep Table
        thresholds = np.arange(0.1, 1.0, 0.1)
        table_data = []
        for t in thresholds:
            y_pred_thresh = (y_probs >= t).astype(int)
            table_data.append({
                "Threshold": f"{t:.1f}",
                "Precision": precision_score(y_test_split, y_pred_thresh),
                "Recall": recall_score(y_test_split, y_pred_thresh),
                "F1-Score": f1_score(y_test_split, y_pred_thresh)
            })
        st.subheader("ğŸšï¸ Threshold Sweep (Precision/Recall/F1)")
        st.table(pd.DataFrame(table_data).set_index("Threshold"))


# --- 4. æ¢ç´¢æ€§è³‡æ–™åˆ†æ (EDA) ---
elif selected == "æ¢ç´¢æ€§è³‡æ–™åˆ†æ":
    st.title("ğŸ“ˆ æ¢ç´¢æ€§è³‡æ–™åˆ†æ (EDA)")

    # --- Data Overview ---
    st.subheader("ğŸ“ è³‡æ–™ç¸½è¦½")
    df['message_len'] = df['message'].apply(len)
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ç¸½è¨Šæ¯æ•¸", len(df))
        ham_skew = df[df['label'] == 'ham']['message_len'].skew()
        st.metric("æ­£å¸¸éƒµä»¶é•·åº¦ååº¦", f"{ham_skew:.2f}")
    with col2:
        st.metric("åƒåœ¾éƒµä»¶èˆ‡æ­£å¸¸éƒµä»¶æ¯”ä¾‹", f"{df['label'].value_counts()['spam']} / {df['label'].value_counts()['ham']}")
        spam_skew = df[df['label'] == 'spam']['message_len'].skew()
        st.metric("åƒåœ¾éƒµä»¶é•·åº¦ååº¦", f"{spam_skew:.2f}")

    st.subheader("ğŸ”¢ è¨Šæ¯é•·åº¦çµ±è¨ˆåˆ†æ")
    st.dataframe(df.groupby('label')['message_len'].describe())

    # --- Class Distribution ---
    st.subheader("ğŸ“Š é¡åˆ¥åˆ†ä½ˆ")
    fig, ax = plt.subplots(figsize=(6, 3))
    sns.countplot(data=df, x='label', ax=ax, palette=['skyblue', 'salmon'])
    ax.set_title("Class Distribution (Bar)")
    st.pyplot(fig)
    
    fig, ax = plt.subplots(figsize=(6, 3))
    df["label"].value_counts().plot(kind='pie', ax=ax, autopct='%1.1f%%', startangle=90, colors=['skyblue', 'salmon'])
    ax.set_ylabel('')
    ax.set_title("Spam vs. Ham")
    st.pyplot(fig)

    # --- Message Length Distribution ---
    st.subheader("ğŸ“ è¨Šæ¯é•·åº¦åˆ†ä½ˆ")
    fig, ax = plt.subplots(figsize=(6, 3))
    sns.kdeplot(data=df, x='message_len', hue='label', fill=True, common_norm=False, palette=["skyblue", "salmon"])
    ax.set_title("Density Plot of Message Length by Class")
    st.pyplot(fig)

    # --- Top Tokens by Class ---
    st.subheader("ğŸ”¥ å„é¡åˆ¥ç†±é–€è©å½™")
    top_n = st.slider("é¸æ“‡ Top-N è©å½™", 5, 25, 10)

    @st.cache_data
    def get_top_tokens(corpus, n=10):
        vec = vectorizer.fit_transform(corpus)
        sum_words = vec.sum(axis=0)
        words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
        return words_freq[:n]

    ham_corpus = df[df['label'] == 'ham']['message']
    spam_corpus = df[df['label'] == 'spam']['message']

    top_ham_tokens = get_top_tokens(ham_corpus, top_n)
    top_spam_tokens = get_top_tokens(spam_corpus, top_n)

    col1, col2 = st.columns(2)
    with col1:
        st.markdown("#### ç†±é–€æ­£å¸¸è©å½™")
        ham_df = pd.DataFrame(top_ham_tokens, columns=['Token', 'Frequency'])
        fig, ax = plt.subplots(figsize=(6, 4))
        sns.barplot(data=ham_df, x='Frequency', y='Token', ax=ax, palette='Greens_r')
        st.pyplot(fig)
    with col2:
        st.markdown("#### ç†±é–€åƒåœ¾è©å½™")
        spam_df = pd.DataFrame(top_spam_tokens, columns=['Token', 'Frequency'])
        fig, ax = plt.subplots(figsize=(6, 4))
        sns.barplot(data=spam_df, x='Frequency', y='Token', ax=ax, palette='Reds_r')
        st.pyplot(fig)

    # --- Word Clouds ---
    st.subheader("â˜ï¸ æ–‡å­—é›²")
    max_words = st.slider("**é¸æ“‡æ–‡å­—é›²ä¸­çš„æœ€å¤§è©å½™æ•¸é‡**", 50, 500, 200, 50)
    
    @st.cache_data
    def generate_wordcloud(text_series, max_words_wc):
        text = " ".join(msg for msg in text_series)
        wordcloud = WordCloud(width=800, height=400, background_color="white", max_words=max_words_wc).generate(text)
        return wordcloud

    col1, col2 = st.columns(2)
    with col1:
        st.markdown("#### Ham (Not Spam)")
        with st.spinner("æ­£åœ¨ç”Ÿæˆ 'Ham' æ–‡å­—é›²..."):
            ham_wc = generate_wordcloud(df[df["label"] == "ham"]["message"], max_words)
            fig, ax = plt.subplots()
            ax.imshow(ham_wc, interpolation='bilinear')
            ax.axis("off")
            st.pyplot(fig)
            
    with col2:
        st.markdown("#### Spam")
        with st.spinner("æ­£åœ¨ç”Ÿæˆ 'Spam' æ–‡å­—é›²..."):
            spam_wc = generate_wordcloud(df[df["label"] == "spam"]["message"], max_words)
            fig, ax = plt.subplots()
            ax.imshow(spam_wc, interpolation='bilinear')
            ax.axis("off")
            st.pyplot(fig)